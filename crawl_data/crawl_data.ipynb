{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFTqF22ZgGB0",
        "outputId": "b303f6c3-9a84-487e-9741-e459d5fdd3b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (2025.4.26)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.1.0 webdriver_manager-4.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install webdriver_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD7eHEjYf9_E",
        "outputId": "aba848cb-5ed1-49a4-b4bd-4feecb9a2ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.32.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM0qDjlnGqHD"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "def get_faculty_info(url):\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'lxml')\n",
        "    view_content_div = soup.find(class_='view-content')\n",
        "    faculty_info = view_content_div.find_all(class_='views-field-field-computed-prof-title')\n",
        "    elements = view_content_div.find_all('h2')\n",
        "    all_hyperlinks = view_content_div.find_all('a', href=True)\n",
        "    emails = []\n",
        "    for link in all_hyperlinks:\n",
        "        if 'mailto:' in link['href']:\n",
        "            emails.append(link['href'].split(':')[1])\n",
        "\n",
        "    #research_area = view_content_div.find_all(class_='views-field-field-research-areas')\n",
        "    research_areas = []\n",
        "    phone_numbers = []\n",
        "    offices = []\n",
        "    for element in faculty_info:\n",
        "        research_area = element.find_next_sibling(class_='views-field-field-research-areas')\n",
        "        phone_number = element.find_next_sibling(class_='views-field views-field-field-computed-phone')\n",
        "        office = element.find_next_sibling(class_='views-field views-field-field-computed-building')\n",
        "        if research_area:\n",
        "            r = research_area.get_text().strip()\n",
        "            if r.startswith('Research Areas:'):\n",
        "                r = r.replace(\"Research Areas:\", \"\").strip()\n",
        "            research_areas.append(r)\n",
        "        else:\n",
        "            research_areas.append(\"\")\n",
        "        if phone_number:\n",
        "            p = phone_number.get_text().strip()\n",
        "            if p.startswith('Phone:'):\n",
        "                p = p.replace(\"Phone:\", \"\").strip()\n",
        "            phone_numbers.append(p)\n",
        "        else:\n",
        "            phone_numbers.append(\"\")\n",
        "        if office:\n",
        "            office_text = office.get_text().strip()\n",
        "            if office_text.startswith(\"Office:\"):\n",
        "                office_text = office_text.replace(\"Office:\", \"\").strip()\n",
        "            offices.append(office_text)\n",
        "        else:\n",
        "            offices.append(\"\")\n",
        "\n",
        "\n",
        "    '''for i in range(len(elements)):\n",
        "        faculty_name = elements[i].get_text()\n",
        "        faculty_position = faculty_info[i].get_text().strip()\n",
        "        faculty_email = emails[i]\n",
        "        print(faculty_name)\n",
        "        print(faculty_position)\n",
        "        print(faculty_email)\n",
        "        print(research_areas[i])\n",
        "        print(offices[i])\n",
        "        print(phone_numbers[i])\n",
        "        print()'''\n",
        "\n",
        "    with open('research_staff_info.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Name', 'Position', 'Email', 'Research Area', 'Office', 'Phone Number'])\n",
        "        for i in range(len(elements)):\n",
        "            faculty_name = elements[i].get_text()\n",
        "            faculty_position = faculty_info[i].get_text().strip()\n",
        "            faculty_email = emails[i]\n",
        "            writer.writerow([faculty_name, faculty_position, faculty_email, research_areas[i], offices[i], phone_numbers[i]])\n",
        "\n",
        "def get_text(url):\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'lxml')\n",
        "    text = soup.get_text()\n",
        "    with open('kiltie_band.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(text)\n",
        "\n",
        "\n",
        "def get_tables(url):\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'lxml')\n",
        "    tables = soup.find_all('table')\n",
        "\n",
        "    for index, table in enumerate(tables, 1):\n",
        "        filename = f\"table_{index}.csv\"\n",
        "        with open(filename, 'w', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            rows = table.find_all('tr')\n",
        "            for row in rows:\n",
        "                columns = row.find_all(['td', 'th'])\n",
        "                row_data = [column.get_text().strip() for column in columns]\n",
        "                writer.writerow(row_data)\n",
        "        print(f\"Table {index} content has been written to {filename} successfully.\")\n",
        "\n",
        "\n",
        "url_faculty_1 = 'https://lti.cs.cmu.edu/directory/all/154/1'\n",
        "url_faculty_2 = 'https://lti.cs.cmu.edu/directory/all/154/1?page=1'\n",
        "url_affiliated_faculty = 'https://lti.cs.cmu.edu/directory/all/154/2728'\n",
        "url_adjunct_faculty = 'https://lti.cs.cmu.edu/directory/all/154/200'\n",
        "lti_staff_1 = 'https://lti.cs.cmu.edu/directory/all/154/2'\n",
        "lti_staff_2 = 'https://lti.cs.cmu.edu/directory/all/154/2?page=1'\n",
        "lti_admin_staff = 'https://lti.cs.cmu.edu/directory/all/154/2731'\n",
        "lti_research_staff = 'https://lti.cs.cmu.edu/directory/all/154/2730'\n",
        "spring_carnival = 'https://web.cvent.com/event/ab7f7aba-4e7c-4637-a1fc-dd1f608702c4/websitePage:645d57e4-75eb-4769-b2c0-f201a0bfc6ce?locale=en' #bad website provided\n",
        "programs = 'https://lti.cs.cmu.edu/learn'\n",
        "spring_carnival_2 = 'https://www.cmu.edu/engage/alumni/events/campus/spring-carnival/schedule/index.html'\n",
        "commencement = 'https://www.cmu.edu/commencement/schedule/index.html'\n",
        "twentyfive_great_things = 'https://www.cs.cmu.edu/scs25/25things'\n",
        "history = 'https://www.cs.cmu.edu/scs25/history'\n",
        "history_2 = 'https://www.cmu.edu/about/history.html'\n",
        "buggy = 'https://www.cmu.edu/news/stories/archives/2019/april/spring-carnival-buggy.html'\n",
        "tartan_facts = 'https://athletics.cmu.edu/athletics/tartanfacts' #TODO: 403 Error - Fix this\n",
        "scotty = 'https://athletics.cmu.edu/athletics/mascot/about'\n",
        "kiltie_band = 'https://athletics.cmu.edu/athletics/kiltieband/index'\n",
        "\n",
        "#get_faculty_info(lti_research_staff)\n",
        "#get_tables(programs)\n",
        "get_text(kiltie_band)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh6EYdn7gN1W"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "\n",
        "def get_workers_info(url):\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'lxml')\n",
        "    #print(soup.prettify())\n",
        "    view_content_div = soup.find(class_='view-content')\n",
        "    elements = view_content_div.find_all('h2')\n",
        "    workers_names = [worker_name.get_text() for worker_name in elements]\n",
        "\n",
        "    return workers_names\n",
        "\n",
        "'''def schedule_of_classes(url):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        html_content= response.text\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        rows = soup.find_all('tr')\n",
        "\n",
        "        with open('Schedule_of_classes.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            csv_writer = csv.writer(csvfile)\n",
        "\n",
        "            for row in rows:\n",
        "                columns = row.find_all('td')\n",
        "                row_data = [column.get_text(strip=True) for column in columns]\n",
        "                csv_writer.writerow(row_data)\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#all urls\n",
        "url_faculty = 'https://lti.cs.cmu.edu/directory/all/154/1'\n",
        "url_affiliated_faculty = 'https://lti.cs.cmu.edu/directory/all/154/2728'\n",
        "#url_adjunct_faculty = 'https://lti.cs.cmu.edu/directory/all/154/200'\n",
        "#url_schedule_of_classes = 'https://enr-apps.as.cmu.edu/assets/SOC/sched_layout_spring.htm'\n",
        "\n",
        "\n",
        "#This will make a csv file of with the timetable of the classes\n",
        "\n",
        "#TODO\n",
        "#this is only for the spring semester. I need FALL, summer data too inside my csv file.\n",
        "#schedule_of_classes(url_schedule_of_classes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTrGBs5GgzVs",
        "outputId": "691d507b-282d-41ab-a287-a089e8888281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.5\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-J2iLAKgkBn",
        "outputId": "f84ca84b-24ca-4a96-c210-333a3ba78013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement get_data (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for get_data\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install get_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD9EDTZNgdcd",
        "outputId": "48c62d4e-a8a6-4ef8-fdec-d6cf1ae84e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting semanticscholar\n",
            "  Downloading semanticscholar-0.10.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from semanticscholar) (9.1.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from semanticscholar) (0.28.1)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from semanticscholar) (1.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->semanticscholar) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->semanticscholar) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->semanticscholar) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->semanticscholar) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->semanticscholar) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->semanticscholar) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->semanticscholar) (4.13.2)\n",
            "Downloading semanticscholar-0.10.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: semanticscholar\n",
            "Successfully installed semanticscholar-0.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install semanticscholar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "Ye_j3t0ygYpq",
        "outputId": "08835067-a7f8-4d59-8702-aed55e2b735a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ralf Brown\n",
            "Yonatan Bisk\n",
            "Ralf Brown\n",
            "Jamie Callan\n",
            "Justine Cassell\n",
            "Mona Diab\n",
            "Fernando Diaz\n",
            "Scott Fahlman\n",
            "Robert Frederking\n",
            "Daniel Fried\n",
            "Anatole Gershman\n",
            "Alexander Hauptmann\n",
            "Daphne Ippolito\n",
            "Lori Levin\n",
            "Lei Li\n",
            "Teruko Mitamura\n",
            "Louis-Philippe Morency\n",
            "David Mortensen\n",
            "Graham Neubig\n",
            "Eric Nyberg\n",
            "Kemal Oflazer\n",
            "Bhiksha Ramakrishnan\n",
            "Carolyn Rosé\n",
            "Alexander Rudnicky\n",
            "Maarten Sap\n",
            "Michael Shamos\n",
            "Rita Singh\n",
            "Emma Strubell\n",
            "Alexander Waibel\n",
            "Shinji Watanabe\n",
            "Sean Welleck\n",
            "Eric P. Xing\n",
            "Chenyan Xiong\n",
            "Yiming Yang\n",
            "Faculty data pulling done\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'json_file_faculties/grahamneubig.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-58d5234cb12c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'paperlinks/grahamneubig'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_authors_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'json_file_faculties/grahamneubig.json'"
          ]
        }
      ],
      "source": [
        "from semanticscholar import SemanticScholar\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "#from get_data import *\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def research_papers(faculty_name):\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/author/search\"\n",
        "    authors_name = faculty_name\n",
        "\n",
        "    query_params = {\n",
        "        \"query\": authors_name,\n",
        "        'year': '2023',\n",
        "        \"fields\": \"paperCount,papers.title,papers.abstract,papers.openAccessPdf,papers.authors,papers.year,papers.publicationVenue,papers.isOpenAccess\"\n",
        "    }\n",
        "\n",
        "\n",
        "    response = requests.get(url, params=query_params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        #time.sleep(1)\n",
        "        data = response.json()\n",
        "        all_authors_data[faculty_name] = data\n",
        "\n",
        "        papers = data.get('papers', [])\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(f\"Request failed with status code {response.status_code}\")\n",
        "\n",
        "\n",
        "def summary_research_papers(json_file_name, text_file_path):\n",
        "    json_url = json_file_name\n",
        "    with open(json_url, \"r\") as file:\n",
        "        json_data = json.load(file)\n",
        "\n",
        "    file_path = text_file_path\n",
        "    with open(file_path, \"w+\", encoding='utf-8') as file:\n",
        "        paper_number = 0\n",
        "        for author, research_paper_data in json_data.items():\n",
        "            for paper_data in research_paper_data[\"data\"]:\n",
        "                for data in paper_data[\"papers\"]:\n",
        "                    if data[\"year\"] == 2023:\n",
        "                        faculty_name = author\n",
        "                        faculty_author_id = paper_data[\"authorId\"]\n",
        "                        paper_title = data[\"title\"]\n",
        "                        if data[\"isOpenAccess\"] == True:\n",
        "                            open_access_bool = data[\"isOpenAccess\"]\n",
        "                            publication_link = data[\"openAccessPdf\"]\n",
        "                            year_published = data[\"year\"]\n",
        "                            paper_abstract = data[\"abstract\"]\n",
        "                            paper_id = data[\"paperId\"]\n",
        "                            if publication_link is None:\n",
        "                                publication_link = 'Its not open access'\n",
        "                                url ='Not given'\n",
        "                            else:\n",
        "                                url = publication_link['url']\n",
        "\n",
        "                            r = requests.post('https://api.semanticscholar.org/graph/v1/paper/batch', params={'fields': 'tldr'}, json={\"ids\": [paper_id]})\n",
        "                            data = r.json()\n",
        "                            if data and isinstance(data, list) and len(data) > 0:\n",
        "                                tldr_info = data[0].get('tldr', {})\n",
        "                                if tldr_info == None:\n",
        "                                    tldr_text = ''\n",
        "                                else:\n",
        "                                    tldr_text = tldr_info.get('text', '')\n",
        "\n",
        "                            file.write(f\"faculty_name: {faculty_name}\\nfaculty_authorid: {faculty_author_id}\\npaper_id: {paper_id}\\npaper_title: {paper_title}\\npublication_link: {url} \\nyear_published: {year_published} \\nabstract_paper:{paper_abstract}\\nisOpenAccess: {open_access_bool}\\nTL\\DR: {tldr_text}\\n================================\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def paper_pdf_links(json_file_name, file_path):\n",
        "    json_url = json_file_name\n",
        "    with open(json_url, \"r\") as file:\n",
        "        json_data = json.load(file)\n",
        "\n",
        "    with open(file_path, \"w+\", encoding='utf-8') as file:\n",
        "        for author, research_paper_data in json_data.items():\n",
        "            for paper_data in research_paper_data[\"data\"]:\n",
        "                for x in paper_data[\"papers\"]:\n",
        "                    if x[\"year\"] == 2023:\n",
        "                        publication_link = x[\"openAccessPdf\"]\n",
        "                        if publication_link is None:\n",
        "                            publication_link = 'Its not open access'\n",
        "                            url ='Not given'\n",
        "                        else:\n",
        "                            url = publication_link['url']\n",
        "                            file.write(f\"{url}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "all_authors_data = {}\n",
        "\n",
        "url_faculty = 'https://web.archive.org/web/20231130214911/https://lti.cs.cmu.edu/directory/all/154/1'\n",
        "url_faculty_page2 = 'https://web.archive.org/web/20231209211749/https://lti.cs.cmu.edu/directory/all/154/1?page=1'\n",
        "\n",
        "faculty_names = get_workers_info(url_faculty)\n",
        "faculty_names = get_workers_info(url_faculty_page2)+faculty_names\n",
        "\n",
        "file_path = '/content/sample_data/faculty_info.csv'\n",
        "file_path_2 = '/content/sample_data/faculty_info_2.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df2 = pd.read_csv(file_path_2)\n",
        "col_name = 'Name'\n",
        "faculty_name1 = df[col_name].tolist()\n",
        "faculty_name2 = df2[col_name].tolist()\n",
        "faculty_names = faculty_name1 + faculty_name2\n",
        "print((faculty_name1[1]))\n",
        "\n",
        "for faculty in faculty_names:\n",
        "    print(faculty)\n",
        "    research_papers(f'{faculty}')\n",
        "\n",
        "\n",
        "\n",
        "#research_papers('Graham Neubig')\n",
        "\n",
        "print('Faculty data pulling done')\n",
        "json_file_name = 'json_file_faculties/grahamneubig.json'\n",
        "text_file_path = 'metadata/grahamneubig'\n",
        "filepath = 'paperlinks/grahamneubig'\n",
        "\n",
        "with open(json_file_name, \"w\") as file:\n",
        "    json.dump(all_authors_data, file, indent=4)\n",
        "\n",
        "print('Created the JSON file')\n",
        "\n",
        "summary_research_papers(json_file_name, text_file_path)\n",
        "print('meta data created')\n",
        "\n",
        "paper_pdf_links(json_file_name, filepath)\n",
        "print('all research papers generated')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_YFJqlunC56"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/Pittsburgh\",\n",
        "    \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\",\n",
        "    \"https://www.pittsburghpa.gov/\",\n",
        "    \"https://www.britannica.com/place/Pittsburgh\",\n",
        "    \"https://pittsburghpa.gov/events\",\n",
        "    \"https://pittsburghsymphony.org/\",\n",
        "    \"https://pittsburgh.events/\",\n",
        "    \"https://downtownpittsburgh.com/events/\",\n",
        "    \"https://www.pghcitypaper.com/pittsburgh/EventSearch?v=d\",\n",
        "    \"https://events.cmu.edu/\",\n",
        "    \"https://www.cmu.edu/engage/alumni/events/campus/index.html\",\n",
        "    \"https://pittsburghopera.org/\",\n",
        "    \"https://trustarts.org/\",\n",
        "    \"https://carnegiemuseums.org/\",\n",
        "    \"https://www.heinzhistorycenter.org/\",\n",
        "    \"https://www.thefrickpittsburgh.org/\",\n",
        "    \"https://en.wikipedia.org/wiki/List_of_museums_in_Pittsburgh\",\n",
        "    \"https://www.visitpittsburgh.com/events-festivals/food-festivals/\",\n",
        "    \"https://www.picklesburgh.com/\",\n",
        "    \"https://www.pghtacofest.com/\",\n",
        "    \"https://pittsburghrestaurantweek.com/\",\n",
        "    \"https://littleitalydays.com/\",\n",
        "    \"https://bananasplitfest.com/\",\n",
        "    \"https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\",\n",
        "    \"https://www.mlb.com/pirates\",\n",
        "    \"https://www.steelers.com/\",\n",
        "    \"https://www.nhl.com/penguins/\",\n",
        "    # ... thêm các URL khác từ danh sách trong ảnh\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvOrs6kQnBzy",
        "outputId": "edc6f55a-dd04-489d-d471-f588d75072bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Crawling: https://en.wikipedia.org/wiki/Pittsburgh ---\n",
            "Pittsburgh (/ˈpɪtsbɜːrɡ/ PITS-burg) is a city in Allegheny County, Pennsylvania, United States, and its county seat. It is the second-most populous city in Pennsylvania (after Philadelphia) and the 68th-most populous city in the U.S., with a population of 302,971 as of the 2020 census. The city is located in southwestern Pennsylvania at the confluence of the Allegheny River and Monongahela River, which combine to form the Ohio River.[7] It anchors the Pittsburgh metropolitan area, which had a po\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://en.wikipedia.org/wiki/History_of_Pittsburgh ---\n",
            "The history of Pittsburgh began with centuries of Native American civilization in the modern Pittsburgh region, known as Jaödeogë’ in the Seneca language.[1] Eventually, European explorers encountered the strategic confluence where the Allegheny and Monongahela Rivers meet to form the Ohio, which leads to the Mississippi River. The area became a battleground when France and Great Britain fought for control in the 1750s. When the British were victorious, the French ceded control of territories ea\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.pittsburghpa.gov/ ---\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Published on May 08, 2025\n",
            "Action on Climate Resilience Following Devastating April 29th Storm\n",
            "Mayor Ed GaineyTakes Action on Climate Resilience Following Devastating April 29th Storm\n",
            "Read Full Story\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Published on May 07, 2025\n",
            "CITY OF PITTSBURGH ANNOUNCES CITIPARKS 2025 FARMERS MARKET SCHEDULE\n",
            "CITY OF PITTSBURGH ANNOUNCESCITIPARKS 2025 FARMERS MARKET SCHEDULE\n",
            "Read Full Story\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Published on May 07, 2025\n",
            "City of Pittsburgh Announces 2025 Pittsburgh World Cup\n",
            "City of Pittsb\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.britannica.com/place/Pittsburgh ---\n",
            "Our editors will review what you’ve submitted and determine whether to revise the article.\n",
            "Pittsburgh,  city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), \n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://pittsburghpa.gov/events ---\n",
            "Error fetching https://pittsburghpa.gov/events: 404 Client Error: Not Found for url: https://www.pittsburghpa.gov/events\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://pittsburghsymphony.org/ ---\n",
            "Error fetching https://pittsburghsymphony.org/: 403 Client Error: Forbidden for url: https://pittsburghsymphony.org/\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://pittsburgh.events/ ---\n",
            "Error fetching https://pittsburgh.events/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://downtownpittsburgh.com/events/ ---\n",
            "Are you hosting an event or activity Downtown? Submit it for inclusion to our calendar for free here. \n",
            "\n",
            "Share this page\n",
            "\n",
            "Contact Us412-566-4190Email Us\n",
            "Pittsburgh Downtown PartnershipThe Bank Tower307 Fourth Avenue – Floor 2Pittsburgh, PA 15222\n",
            "\n",
            "Share this page\n",
            "Events\n",
            "Events\n",
            "See what’s happening in Downtown Pittsburgh with the official neighborhood calendar.\n",
            "May\n",
            "May 2025\n",
            "June 2025\n",
            "Sensory Friendly Studio Night (18+): POP in Motion\n",
            "The Moon Show\n",
            "The Latchkey Kids\n",
            "Trevor Wallace Post Show Meet & G\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.pghcitypaper.com/pittsburgh/EventSearch?v=d ---\n",
            "\n",
            "\n",
            "Promoted\n",
            "\n",
            "\n",
            "\n",
            "Promoted\n",
            "\n",
            "\n",
            "        668 \n",
            "        \n",
            "          results\n",
            "        \n",
            "      \n",
            "\n",
            " page 1\n",
            " of 23\n",
            "\n",
            "\n",
            "Duo Junction in Concert (Jack Kurutz and Becky Billock, pianists)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            Sun., May 18, 1-2:30 p.m.\n",
            "          \n",
            "\n",
            "PNC Recital Hall, Duquesne Univ.\n",
            "\n",
            "\n",
            "\n",
            "                600 Forbes Ave., Pittsburgh\n",
            "              \n",
            "\n",
            "                  Uptown\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Get Tickets \n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Classical, Dance\n",
            "\n",
            "\n",
            "Jazz Poetry 2025: Jerome Jennings & iLL Philosophy, Carly Ing\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://events.cmu.edu/ ---\n",
            "\n",
            "          Events can be submitted by any CMU community member with an Andrew ID. All submissions will be reviewed before posting to the calendar.\n",
            "        \n",
            "\n",
            "      Carnegie Mellon University\n",
            "      5000 Forbes Avenue\n",
            "      Pittsburgh, PA 15213\n",
            "      412-268-2000\n",
            "    \n",
            "          Find Events\n",
            "        \n",
            "\n",
            "          Submit Events\n",
            "        \n",
            "\n",
            "          Helpful Links\n",
            "        \n",
            "\n",
            "Submit an Event\n",
            "\n",
            "\n",
            "Get Help or Request an Edit\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Academic Calendar\n",
            "\n",
            "\n",
            "Admission Events\n",
            "\n",
            "\n",
            "Alumni Events\n",
            "\n",
            "\n",
            "Athletics Schedule\n",
            "\n",
            "\n",
            "Commence\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.cmu.edu/engage/alumni/events/campus/index.html ---\n",
            "They are Carnegie Mellon’s most cherished traditions and part of the fabric that weaves the many generations of Tartans together. No matter your class year, these events are just so CMU.\n",
            "Spring CarnivalFor more than 100 years, Spring Carnival has been one of the most anticipated weekends of the year. Tartans from around the world return to campus to carry on events like Booth, Buggy, MOBOT, Scotch’n’Soda performances and much more.\n",
            "Homecoming Weekend\n",
            "Each fall, the CMU community puts on their be\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://pittsburghopera.org/ ---\n",
            "SCARLET SOIREE\n",
            "This afterparty is part of Maecenas XL: Opera in Red, our year-end celebration gala, which has been hailed as ‘one of the best black tie galas in town’ and ‘one of Pittsburgh’s top parties of the year’... and this year we are turning up the heat! \n",
            "MONTEREY BAY FISH GROTTOSATURDAY, MAY 17\n",
            "SCARLET SOIREE\n",
            "This afterparty is part of Maecenas XL: Opera in Red, our year-end celebration gala, which has been hailed as ‘one of the best black tie galas in town’ and ‘one of Pittsburgh’s top \n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://trustarts.org/ ---\n",
            "Error fetching https://trustarts.org/: 403 Client Error: Forbidden for url: https://trustarts.org/\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://carnegiemuseums.org/ ---\n",
            "Error fetching https://carnegiemuseums.org/: 403 Client Error: Forbidden for url: https://carnegiemuseums.org/\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.heinzhistorycenter.org/ ---\n",
            "Error fetching https://www.heinzhistorycenter.org/: 403 Client Error: Forbidden for url: https://www.heinzhistorycenter.org/\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.thefrickpittsburgh.org/ ---\n",
            "As spring blooms, come and enjoy the warm weather on our lush grounds while exploring history and art! We have something for everyone — dig deeper into 1892 Pittsburgh through our award-winning Gilded, Not Golden tour of Clayton, sip on specialty lemonades from The Café at the Frick and experience Kara Walker: Harper's Pictorial History of the Civil War (Annotated) before it closes on May 25. Plus, learn about the 20th-century automobile industry through our Car and Carriage Museum and bask in t\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://en.wikipedia.org/wiki/List_of_museums_in_Pittsburgh ---\n",
            "This list of museums in Pittsburgh, Pennsylvania encompasses museums defined for this context as institutions (including nonprofit organizations, government entities, and private businesses) that collect and care for objects of cultural, artistic, scientific, or historical interest and make their collections or related exhibits available for public viewing. Also included are university and non-profit art galleries.  Museums that exist only in cyberspace (i.e., virtual museums) are not included.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.visitpittsburgh.com/events-festivals/food-festivals/ ---\n",
            "Food and fun just go together and, in Pittsburgh, food is often the reason for the fun.\n",
            "There's a Pittsburgh Restaurant Week celebration in the winter and summer, neighborhood soup crawls and cookie tours, apple, maple and whiskey festivals. And, Pittsburgh's annual food festivals feature ethnic or traditional foods as well as the music, dance, crafts and culture that go along with them.\n",
            "Winter: Jan. 15-21, 2024Summer: Aug. 12-18, 2024\n",
            "Pittsburgh Restaurant Week in January and August each year f\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.picklesburgh.com/ ---\n",
            "Error fetching https://www.picklesburgh.com/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.pghtacofest.com/ ---\n",
            "2875 Railroad St, Pittsburgh, PA 15222\n",
            "PR@pghtacofest.comPittsburgh Taco FestivalÂ®\n",
            "follow us on\n",
            "sponsored by\n",
            "2025 Pittsburgh Taco FestivalÂ® Venue\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://pittsburghrestaurantweek.com/ ---\n",
            "Be social and RSVP to the official Facebook event\n",
            "Let chance select your next restaurant week destinationPittsburgh Restaurant Week\n",
            "Most Delicious Time Of YearGet Involved \n",
            "\n",
            "Join Newsletterregister for restaurant week updates * indicates required Email Address * First Name Last Name Zip Code Close var fnames = new Array();var ftypes = new Array();fnames[0]=’EMAIL’;ftypes[0]=’email’;fnames[1]=’FNAME’;ftypes[1]=’text’;fnames[2]=’LNAME’;ftypes[2]=’text’;fnames[3]=’ZIP_CODE’;ftypes[3]=’number’; try \n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://littleitalydays.com/ ---\n",
            "Error fetching https://littleitalydays.com/: 403 Client Error: Forbidden for url: https://littleitalydays.com/\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://bananasplitfest.com/ ---\n",
            "August 23-24, 2025\n",
            "Sign up to be a featured part of the fun! We love supporting local vendors. Come be part of one of the biggest events of the year!\n",
            "The Great American Banana Split Celebration couldn’t have created this weekend without the help of our wonderful sponsors. \n",
            "Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n",
            "Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n",
            "Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n",
            "We have activities for the whole family! Enjoy activ\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/ ---\n",
            "In Pittsburgh, we bleed black and gold.\n",
            "Come see for yourself why Sporting News magazine awarded Pittsburgh the coveted \"Best Sports City\" title and why the USA TODAY 10 Best Reader's Choice poll named Pittsburgh as one of the winners of the \"Best City for Sports\" travel award.\n",
            "If its action you want, this city has it covered with the best of football, baseball, hockey and more. Grab your Terrible Towel and visit Acrisure Stadium to watch the six-time Super Bowl Champion Pittsburgh Steelers. Hea\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.mlb.com/pirates ---\n",
            "TicketsSingle Game TicketsMini PlansSeason TicketsSpecial Ticket EventsGroup TicketsFan Value DealsPittsburgh Baseball Club LevelPremium SeatingHospitality AreasMy Pirates TicketsDigital Ticketing3D Seating MapPNC Park ToursConcerts & EventsMLB Ballpark AppBuy & Sell Tickets on SeatGeek2025 Ticket Info & SchedulePremiumSchedule2025 Regular Season SchedulePromotions ScheduleSortable SchedulePrintable ScheduleDownloadable ScheduleBroadcast ScheduleMLB EventsScoresStatsTeam StatsTop Prospect StatsA\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.steelers.com/ ---\n",
            "The Steelers rookie class arrived at the UPMC Rooney Sports Complex on Thursday, ready for the team's three-day minicamp\n",
            "The Steelers 2025 rookies arrived at the UPMC Rooney Sports Complex in preparation for rookie minicamp\n",
            "STEELERS PRO SHOP\n",
            "2025 KEY DATES\n",
            "2025 OPPONENTS\n",
            "DUBLIN GAME\n",
            "PLAY YINZCHAT!\n",
            "HALL OF HONOR MUSEUM\n",
            "YinzChat Schedule Release Challenge\n",
            "Predict our 2025 schedule with YinzChat starting May 7th! Answer predictive questions each day for your chance to win a T.J. Watt Autographed Fo\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Crawling: https://www.nhl.com/penguins/ ---\n",
            "\n",
            "\n",
            "Treat Mom! Special Ticket Package Includes Tumbler. BUY NOW\n",
            "\n",
            "\n",
            "Bold Penguin Challenge: Bloopers\n",
            "Tomasino speaks to the media\n",
            "Shea speaks to the media\n",
            "Ponomarev speaks to the media\n",
            "Nedeljkovic speaks to the media\n",
            "Koivunen speaks to the media\n",
            "Jarry speaks to the media\n",
            "Sullivan speaks to the media\n",
            "Dewar speaks to the media\n",
            "Rust speaks to the media\n",
            "Rakell speaks to the media\n",
            "McGroarty speaks to the media\n",
            "Lizotte speaks to the media\n",
            "Grzelcyk speaks to the media\n",
            "Karlsson speaks to the media\n",
            "Crosby sp\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def crawl_page(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Lấy tất cả các đoạn văn, tiêu đề, và danh sách (và có thể điều chỉnh thêm)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        headers = soup.find_all(['h1', 'h2', 'h3'])\n",
        "        lists = soup.find_all('ul')\n",
        "\n",
        "        # Kết hợp nội dung từ các đoạn văn, tiêu đề và danh sách\n",
        "        content = \"\\n\".join(p.get_text() for p in paragraphs if p.get_text().strip())\n",
        "        content += \"\\n\".join(h.get_text() for h in headers if h.get_text().strip())\n",
        "        content += \"\\n\".join(li.get_text() for li in lists if li.get_text().strip())\n",
        "\n",
        "        return content[:100000]  # Tăng độ dài nội dung nếu cần\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching {url}: {e}\"\n",
        "\n",
        "with open(\"crawled_pages.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for url in urls:\n",
        "        print(f\"--- Crawling: {url} ---\")\n",
        "        content = crawl_page(url)\n",
        "        # Ghi vào file chỉ nội dung, không URL\n",
        "        f.write(content)\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\\n\")\n",
        "        # In trước 500 ký tự đầu tiên để xem\n",
        "        print(content[:500])\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl28WoKIXFha",
        "outputId": "5caf305a-bcea-4582-b5ee-8288fe0733c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/303.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "XvlEqRksW-YM",
        "outputId": "249895b0-56eb-44a7-996b-1b63b4d0a48d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../../data/raw/raw_pdf_data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3df1cbd4ffc2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Get all PDF files in the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mpdf_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_pdf_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Process each PDF file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3df1cbd4ffc2>\u001b[0m in \u001b[0;36mlist_pdf_files\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Function to list all PDF files in a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlist_pdf_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/raw/raw_pdf_data'"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Script for pdf crawling and text extraction\n",
        "'''\n",
        "\n",
        "import os\n",
        "from pypdf import PdfReader\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    # Create a PDF reader object\n",
        "    reader = PdfReader(pdf_path)\n",
        "\n",
        "    # Initialize a string to hold the extracted text\n",
        "    extracted_text = \"\"\n",
        "\n",
        "    # Loop through each page in the PDF\n",
        "    for page_num in range(len(reader.pages)):\n",
        "        page = reader.pages[page_num]\n",
        "        extracted_text += page.extract_text() + \"\\n\"  # Extract text from each page and add newline for separation\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "# Function to list all PDF files in a directory\n",
        "def list_pdf_files(directory):\n",
        "    return [f for f in os.listdir(directory) if f.endswith('.pdf')]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Directory containing the PDF files\n",
        "    pdf_file_dir = '../../data/raw/raw_pdf_data'\n",
        "\n",
        "    # Get all PDF files in the directory\n",
        "    pdf_files = list_pdf_files(pdf_file_dir)\n",
        "\n",
        "    # Process each PDF file\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_file_path = os.path.join(pdf_file_dir, pdf_file)  # Full path to the PDF file\n",
        "        print(f\"Processing {pdf_file}...\")\n",
        "\n",
        "        # Extract text from the PDF\n",
        "        text = extract_text_from_pdf(pdf_file_path)\n",
        "\n",
        "        # Create a corresponding text file name (same name as the PDF, but with .txt extension)\n",
        "        txt_file_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
        "        txt_file_path = os.path.join('../../data/crawled/crawled_pdf_text_data', txt_file_name)\n",
        "\n",
        "        # Save the extracted text to a text file\n",
        "        with open(txt_file_path, 'w', encoding='utf-8') as text_file:\n",
        "            cleaned_text = text.replace('\\n', ' ')\n",
        "            text_file.write(cleaned_text)\n",
        "\n",
        "        print(f\"Saved extracted text to {txt_file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-C1r4XjYIzb",
        "outputId": "a24bd05e-f130-46bc-e978-caf660fe9d4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmyffdAoYxi9",
        "outputId": "c83df0ef-5512-4c85-9663-38e58afc0ca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔎 Đang crawl: https://en.wikipedia.org/wiki/Pittsburgh\n",
            "✅ Tìm thấy 17 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://en.wikipedia.org/wiki/History_of_Pittsburgh\n",
            "✅ Tìm thấy 2 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.pittsburghpa.gov/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.britannica.com/place/Pittsburgh\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://pittsburghpa.gov/events\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://pittsburghsymphony.org/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://pittsburgh.events/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://downtownpittsburgh.com/events/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.pghcitypaper.com/pittsburgh/EventSearch?v=d\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://events.cmu.edu/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.cmu.edu/engage/alumni/events/campus/index.html\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://pittsburghopera.org/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://trustarts.org/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://carnegiemuseums.org/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.heinzhistorycenter.org/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.thefrickpittsburgh.org/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://en.wikipedia.org/wiki/List_of_museums_in_Pittsburgh\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.visitpittsburgh.com/events-festivals/food-festivals/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.picklesburgh.com/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.pghtacofest.com/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://pittsburghrestaurantweek.com/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://littleitalydays.com/\n",
            "✅ Tìm thấy 3 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://bananasplitfest.com/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.mlb.com/pirates\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.steelers.com/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "🔎 Đang crawl: https://www.nhl.com/penguins/\n",
            "✅ Tìm thấy 0 link PDF\n",
            "\n",
            "📄 TỔNG SỐ LINK PDF: 22\n",
            "http://www.nsf.gov/statistics/infbrief/nsf13305/nsf13305.pdf\n",
            "http://174.143.38.57/wp-content/uploads/2010/06/S013_ROBOT-RxSellSheet.pdf\n",
            "http://www.co.greene.pa.us/secured/gc2/history/Struggle-for-Possession.pdf\n",
            "https://upittpress.org/wp-content/uploads/2019/01/9780822943716exr.pdf\n",
            "https://opapgh.org/wp-content/uploads/2020/08/OPA_ArtinPublicPlaces_RetailFirstside.pdf\n",
            "https://www.weather.gov/media/pbz/records/prec.pdf\n",
            "https://www2.census.gov/library/publications/decennial/1980/volume-1/pennsylvania/1980censusofpopu80140un_bw.pdf\n",
            "https://www2.census.gov/library/publications/decennial/1990/cp-1/cp-1-40-1.pdf\n",
            "https://www.americanimmigrationcouncil.org/sites/default/files/research/council_new_americans_in_pittsburgh_9_2023.pdf\n",
            "https://web.archive.org/web/20191221235539/https://www.brandeis.edu/ssri/pdfs/communitystudies/PittsburghJewishCommStudy.pdf\n",
            "https://www.brandeis.edu/ssri/pdfs/communitystudies/PittsburghJewishCommStudy.pdf\n",
            "https://web.archive.org/web/20140202184957/http://obs.rc.fas.harvard.edu/chetty/mobility_geo.pdf\n",
            "https://www.ceapittsburgh.org/wp-content/uploads/2020/01/A-Ground-Up-Model-for-Gun-Violence-Reduction.pdf\n",
            "https://aeg.memberclicks.net/assets/docs/Geology%20of%20Pittsburgh%20Book.pdf\n",
            "https://web.archive.org/web/20070927220849/http://www.apta.com/research/stats/ridership/riderep/documents/07q1bus.pdf\n",
            "http://www.apta.com/research/stats/ridership/riderep/documents/07q1bus.pdf\n",
            "http://aapa.files.cms-plus.com/PDFs/2011%20U%20S%20%20PORT%20RANKINGS%20BY%20CARGO%20TONNAGE.pdf\n",
            "http://docs.lib.noaa.gov/rescue/mwr/055/mwr-055-11-0500a.pdf\n",
            "http://www.co.greene.pa.us/secured/gc2/history/Struggle-for-Possession.pdf\n",
            "http://littleitalydays.com/wp-content/uploads/2024/07/MISS-LITTLE-ITALY-2024.pdf\n",
            "http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEntryForm.pdf\n",
            "http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEventOverview.pdf\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Hàm crawl PDF từ 1 URL\n",
        "def find_pdf_links_from_url(base_url):\n",
        "    try:\n",
        "        response = requests.get(base_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        pdf_links = []\n",
        "\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            if href.endswith('.pdf'):\n",
        "                full_url = urljoin(base_url, href)  # nối nếu link tương đối\n",
        "                pdf_links.append(full_url)\n",
        "\n",
        "        return pdf_links\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi với {base_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Danh sách URL cần crawl\n",
        "url_list = [\n",
        "    \"https://en.wikipedia.org/wiki/Pittsburgh\",\n",
        "    \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\",\n",
        "    \"https://www.pittsburghpa.gov/\",\n",
        "    \"https://www.britannica.com/place/Pittsburgh\",\n",
        "    \"https://pittsburghpa.gov/events\",\n",
        "    \"https://pittsburghsymphony.org/\",\n",
        "    \"https://pittsburgh.events/\",\n",
        "    \"https://downtownpittsburgh.com/events/\",\n",
        "    \"https://www.pghcitypaper.com/pittsburgh/EventSearch?v=d\",\n",
        "    \"https://events.cmu.edu/\",\n",
        "    \"https://www.cmu.edu/engage/alumni/events/campus/index.html\",\n",
        "    \"https://pittsburghopera.org/\",\n",
        "    \"https://trustarts.org/\",\n",
        "    \"https://carnegiemuseums.org/\",\n",
        "    \"https://www.heinzhistorycenter.org/\",\n",
        "    \"https://www.thefrickpittsburgh.org/\",\n",
        "    \"https://en.wikipedia.org/wiki/List_of_museums_in_Pittsburgh\",\n",
        "    \"https://www.visitpittsburgh.com/events-festivals/food-festivals/\",\n",
        "    \"https://www.picklesburgh.com/\",\n",
        "    \"https://www.pghtacofest.com/\",\n",
        "    \"https://pittsburghrestaurantweek.com/\",\n",
        "    \"https://littleitalydays.com/\",\n",
        "    \"https://bananasplitfest.com/\",\n",
        "    \"https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\",\n",
        "    \"https://www.mlb.com/pirates\",\n",
        "    \"https://www.steelers.com/\",\n",
        "    \"https://www.nhl.com/penguins/\",\n",
        "    # ... thêm các URL khác từ danh sách trong ảnh\n",
        "]\n",
        "\n",
        "# Tìm tất cả link PDF từ các URL\n",
        "all_pdf_links = []\n",
        "\n",
        "for url in url_list:\n",
        "    print(f\"🔎 Đang crawl: {url}\")\n",
        "    pdfs = find_pdf_links_from_url(url)\n",
        "    all_pdf_links.extend(pdfs)\n",
        "    print(f\"✅ Tìm thấy {len(pdfs)} link PDF\\n\")\n",
        "\n",
        "# In kết quả\n",
        "print(\"📄 TỔNG SỐ LINK PDF:\", len(all_pdf_links))\n",
        "for link in all_pdf_links:\n",
        "    print(link)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vitq_mDLZHej",
        "outputId": "a44ab2fd-05ae-4ae2-f318-4fd850bd8269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4BWiC8FcN_a",
        "outputId": "b358092b-adc4-42ee-9345-1b16dabb5e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔽 Đang xử lý file 1: http://www.nsf.gov/statistics/infbrief/nsf13305/nsf13305.pdf\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: http://www.nsf.gov/statistics/infbrief/nsf13305/nsf13305.pdf\n",
            "\n",
            "🔽 Đang xử lý file 2: http://174.143.38.57/wp-content/uploads/2010/06/S013_ROBOT-RxSellSheet.pdf\n",
            "❌ Lỗi khi tải http://174.143.38.57/wp-content/uploads/2010/06/S013_ROBOT-RxSellSheet.pdf: HTTPConnectionPool(host='174.143.38.57', port=80): Max retries exceeded with url: /wp-content/uploads/2010/06/S013_ROBOT-RxSellSheet.pdf (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fe815428ed0>, 'Connection to 174.143.38.57 timed out. (connect timeout=10)'))\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: http://174.143.38.57/wp-content/uploads/2010/06/S013_ROBOT-RxSellSheet.pdf\n",
            "\n",
            "🔽 Đang xử lý file 3: http://www.co.greene.pa.us/secured/gc2/history/Struggle-for-Possession.pdf\n",
            "❌ Lỗi khi tải http://www.co.greene.pa.us/secured/gc2/history/Struggle-for-Possession.pdf: HTTPConnectionPool(host='www.co.greene.pa.us', port=80): Max retries exceeded with url: /secured/gc2/history/Struggle-for-Possession.pdf (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fe81626b7d0>, 'Connection to www.co.greene.pa.us timed out. (connect timeout=10)'))\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: http://www.co.greene.pa.us/secured/gc2/history/Struggle-for-Possession.pdf\n",
            "\n",
            "🔽 Đang xử lý file 4: https://upittpress.org/wp-content/uploads/2019/01/9780822943716exr.pdf\n",
            "✅ Đã lưu file: downloaded_pdfs/file_4.pdf\n",
            "📝 Đã trích xuất và lưu văn bản vào: extracted_texts/file_4.txt\n",
            "\n",
            "🔽 Đang xử lý file 5: https://opapgh.org/wp-content/uploads/2020/08/OPA_ArtinPublicPlaces_RetailFirstside.pdf\n",
            "❌ Không thể tải: https://opapgh.org/wp-content/uploads/2020/08/OPA_ArtinPublicPlaces_RetailFirstside.pdf — Mã lỗi: 403\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: https://opapgh.org/wp-content/uploads/2020/08/OPA_ArtinPublicPlaces_RetailFirstside.pdf\n",
            "\n",
            "🔽 Đang xử lý file 6: https://www.weather.gov/media/pbz/records/prec.pdf\n",
            "✅ Đã lưu file: downloaded_pdfs/file_6.pdf\n",
            "📝 Đã trích xuất và lưu văn bản vào: extracted_texts/file_6.txt\n",
            "\n",
            "🔽 Đang xử lý file 7: https://www2.census.gov/library/publications/decennial/1980/volume-1/pennsylvania/1980censusofpopu80140un_bw.pdf\n",
            "❌ Lỗi khi tải https://www2.census.gov/library/publications/decennial/1980/volume-1/pennsylvania/1980censusofpopu80140un_bw.pdf: HTTPSConnectionPool(host='www2.census.gov', port=443): Max retries exceeded with url: /library/publications/decennial/1980/volume-1/pennsylvania/1980censusofpopu80140un_bw.pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: https://www2.census.gov/library/publications/decennial/1980/volume-1/pennsylvania/1980censusofpopu80140un_bw.pdf\n",
            "\n",
            "🔽 Đang xử lý file 8: https://www2.census.gov/library/publications/decennial/1990/cp-1/cp-1-40-1.pdf\n",
            "❌ Lỗi khi tải https://www2.census.gov/library/publications/decennial/1990/cp-1/cp-1-40-1.pdf: HTTPSConnectionPool(host='www2.census.gov', port=443): Max retries exceeded with url: /library/publications/decennial/1990/cp-1/cp-1-40-1.pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: https://www2.census.gov/library/publications/decennial/1990/cp-1/cp-1-40-1.pdf\n",
            "\n",
            "🔽 Đang xử lý file 9: https://www.americanimmigrationcouncil.org/sites/default/files/research/council_new_americans_in_pittsburgh_9_2023.pdf\n",
            "❌ Không thể tải: https://www.americanimmigrationcouncil.org/sites/default/files/research/council_new_americans_in_pittsburgh_9_2023.pdf — Mã lỗi: 403\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: https://www.americanimmigrationcouncil.org/sites/default/files/research/council_new_americans_in_pittsburgh_9_2023.pdf\n",
            "\n",
            "🔽 Đang xử lý file 10: https://web.archive.org/web/20191221235539/https://www.brandeis.edu/ssri/pdfs/communitystudies/PittsburghJewishCommStudy.pdf\n",
            "✅ Đã lưu file: downloaded_pdfs/file_10.pdf\n",
            "📝 Đã trích xuất và lưu văn bản vào: extracted_texts/file_10.txt\n",
            "\n",
            "🔽 Đang xử lý file 11: https://www.brandeis.edu/ssri/pdfs/communitystudies/PittsburghJewishCommStudy.pdf\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: https://www.brandeis.edu/ssri/pdfs/communitystudies/PittsburghJewishCommStudy.pdf\n",
            "\n",
            "🔽 Đang xử lý file 12: https://web.archive.org/web/20140202184957/http://obs.rc.fas.harvard.edu/chetty/mobility_geo.pdf\n",
            "✅ Đã lưu file: downloaded_pdfs/file_12.pdf\n",
            "📝 Đã trích xuất và lưu văn bản vào: extracted_texts/file_12.txt\n",
            "\n",
            "🔽 Đang xử lý file 13: https://www.ceapittsburgh.org/wp-content/uploads/2020/01/A-Ground-Up-Model-for-Gun-Violence-Reduction.pdf\n",
            "❌ Không thể tải: https://www.ceapittsburgh.org/wp-content/uploads/2020/01/A-Ground-Up-Model-for-Gun-Violence-Reduction.pdf — Mã lỗi: 404\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: https://www.ceapittsburgh.org/wp-content/uploads/2020/01/A-Ground-Up-Model-for-Gun-Violence-Reduction.pdf\n",
            "\n",
            "🔽 Đang xử lý file 14: https://aeg.memberclicks.net/assets/docs/Geology%20of%20Pittsburgh%20Book.pdf\n",
            "✅ Đã lưu file: downloaded_pdfs/file_14.pdf\n",
            "📝 Đã trích xuất và lưu văn bản vào: extracted_texts/file_14.txt\n",
            "\n",
            "🔽 Đang xử lý file 15: https://web.archive.org/web/20070927220849/http://www.apta.com/research/stats/ridership/riderep/documents/07q1bus.pdf\n",
            "✅ Đã lưu file: downloaded_pdfs/file_15.pdf\n",
            "📝 Đã trích xuất và lưu văn bản vào: extracted_texts/file_15.txt\n",
            "\n",
            "🔽 Đang xử lý file 16: http://www.apta.com/research/stats/ridership/riderep/documents/07q1bus.pdf\n",
            "❌ Không thể tải: http://www.apta.com/research/stats/ridership/riderep/documents/07q1bus.pdf — Mã lỗi: 404\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: http://www.apta.com/research/stats/ridership/riderep/documents/07q1bus.pdf\n",
            "\n",
            "🔽 Đang xử lý file 17: http://aapa.files.cms-plus.com/PDFs/2011%20U%20S%20%20PORT%20RANKINGS%20BY%20CARGO%20TONNAGE.pdf\n",
            "✅ Đã lưu file: downloaded_pdfs/file_17.pdf\n",
            "📝 Đã trích xuất và lưu văn bản vào: extracted_texts/file_17.txt\n",
            "\n",
            "🔽 Đang xử lý file 18: http://docs.lib.noaa.gov/rescue/mwr/055/mwr-055-11-0500a.pdf\n",
            "❌ Lỗi khi tải http://docs.lib.noaa.gov/rescue/mwr/055/mwr-055-11-0500a.pdf: HTTPConnectionPool(host='docs.lib.noaa.gov', port=80): Max retries exceeded with url: /rescue/mwr/055/mwr-055-11-0500a.pdf (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7fe8148a4dd0>: Failed to resolve 'docs.lib.noaa.gov' ([Errno -2] Name or service not known)\"))\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: http://docs.lib.noaa.gov/rescue/mwr/055/mwr-055-11-0500a.pdf\n",
            "\n",
            "🔽 Đang xử lý file 19: http://littleitalydays.com/wp-content/uploads/2024/07/MISS-LITTLE-ITALY-2024.pdf\n",
            "❌ Không thể tải: http://littleitalydays.com/wp-content/uploads/2024/07/MISS-LITTLE-ITALY-2024.pdf — Mã lỗi: 403\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: http://littleitalydays.com/wp-content/uploads/2024/07/MISS-LITTLE-ITALY-2024.pdf\n",
            "\n",
            "🔽 Đang xử lý file 20: http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEntryForm.pdf\n",
            "❌ Không thể tải: http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEntryForm.pdf — Mã lỗi: 403\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEntryForm.pdf\n",
            "\n",
            "🔽 Đang xử lý file 21: http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEventOverview.pdf\n",
            "❌ Không thể tải: http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEventOverview.pdf — Mã lỗi: 403\n",
            "⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEventOverview.pdf\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from pypdf import PdfReader\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Thư mục lưu\n",
        "os.makedirs(\"downloaded_pdfs\", exist_ok=True)\n",
        "os.makedirs(\"extracted_texts\", exist_ok=True)\n",
        "\n",
        "# 1. Danh sách các URL PDF\n",
        "pdf_urls = [\n",
        "    \"http://www.nsf.gov/statistics/infbrief/nsf13305/nsf13305.pdf\",\n",
        "    \"http://174.143.38.57/wp-content/uploads/2010/06/S013_ROBOT-RxSellSheet.pdf\",\n",
        "    \"http://www.co.greene.pa.us/secured/gc2/history/Struggle-for-Possession.pdf\",\n",
        "    \"https://upittpress.org/wp-content/uploads/2019/01/9780822943716exr.pdf\",\n",
        "    \"https://opapgh.org/wp-content/uploads/2020/08/OPA_ArtinPublicPlaces_RetailFirstside.pdf\",\n",
        "    \"https://www.weather.gov/media/pbz/records/prec.pdf\",\n",
        "    \"https://www2.census.gov/library/publications/decennial/1980/volume-1/pennsylvania/1980censusofpopu80140un_bw.pdf\",\n",
        "    \"https://www2.census.gov/library/publications/decennial/1990/cp-1/cp-1-40-1.pdf\",\n",
        "    \"https://www.americanimmigrationcouncil.org/sites/default/files/research/council_new_americans_in_pittsburgh_9_2023.pdf\",\n",
        "    \"https://web.archive.org/web/20191221235539/https://www.brandeis.edu/ssri/pdfs/communitystudies/PittsburghJewishCommStudy.pdf\",\n",
        "    \"https://www.brandeis.edu/ssri/pdfs/communitystudies/PittsburghJewishCommStudy.pdf\",\n",
        "    \"https://web.archive.org/web/20140202184957/http://obs.rc.fas.harvard.edu/chetty/mobility_geo.pdf\",\n",
        "    \"https://www.ceapittsburgh.org/wp-content/uploads/2020/01/A-Ground-Up-Model-for-Gun-Violence-Reduction.pdf\",\n",
        "    \"https://aeg.memberclicks.net/assets/docs/Geology%20of%20Pittsburgh%20Book.pdf\",\n",
        "    \"https://web.archive.org/web/20070927220849/http://www.apta.com/research/stats/ridership/riderep/documents/07q1bus.pdf\",\n",
        "    \"http://www.apta.com/research/stats/ridership/riderep/documents/07q1bus.pdf\",\n",
        "    \"http://aapa.files.cms-plus.com/PDFs/2011%20U%20S%20%20PORT%20RANKINGS%20BY%20CARGO%20TONNAGE.pdf\",\n",
        "    \"http://docs.lib.noaa.gov/rescue/mwr/055/mwr-055-11-0500a.pdf\",\n",
        "    \"http://littleitalydays.com/wp-content/uploads/2024/07/MISS-LITTLE-ITALY-2024.pdf\",\n",
        "    \"http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEntryForm.pdf\",\n",
        "    \"http://littleitalydays.com/wp-content/uploads/2019/07/GolffatherEventOverview.pdf\"\n",
        "]\n",
        "\n",
        "\n",
        "# 2. Kiểm tra xem file có phải PDF thật không\n",
        "def is_valid_pdf(file_bytes):\n",
        "    file_bytes.seek(0)\n",
        "    return file_bytes.read(4) == b'%PDF'\n",
        "\n",
        "# 3. Tải file PDF từ URL\n",
        "def download_pdf_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return io.BytesIO(response.content)\n",
        "        else:\n",
        "            print(f\"❌ Không thể tải: {url} — Mã lỗi: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Lỗi khi tải {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# 4. Trích xuất text từ PDF\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# 5. Lặp qua từng URL, tải và trích xuất text\n",
        "for i, url in enumerate(pdf_urls):\n",
        "    print(f\"\\n🔽 Đang xử lý file {i+1}: {url}\")\n",
        "    pdf_file = download_pdf_from_url(url)\n",
        "\n",
        "    if pdf_file and is_valid_pdf(pdf_file):\n",
        "        pdf_filename = f\"downloaded_pdfs/file_{i+1}.pdf\"\n",
        "        text_filename = f\"extracted_texts/file_{i+1}.txt\"\n",
        "\n",
        "        # Lưu file PDF\n",
        "        with open(pdf_filename, 'wb') as f:\n",
        "            f.write(pdf_file.getbuffer())\n",
        "        print(f\"✅ Đã lưu file: {pdf_filename}\")\n",
        "\n",
        "        # Trích xuất và lưu text\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "        with open(text_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        print(f\"📝 Đã trích xuất và lưu văn bản vào: {text_filename}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"⚠️ Bỏ qua: File không hợp lệ hoặc không phải PDF: {url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz1k9dzoq2Do",
        "outputId": "45ab9752-b769-4a30-e6cc-0a05cd00066c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Đã gộp 8 file vào: all_texts_combined.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Thư mục chứa các file .txt đã trích xuất\n",
        "text_dir = \"/content/extracted_texts\"\n",
        "output_file = \"all_texts_combined.txt\"\n",
        "\n",
        "# Lấy danh sách tất cả các file .txt trong thư mục\n",
        "text_files = sorted([f for f in os.listdir(text_dir) if f.endswith('.txt')])\n",
        "\n",
        "# Gộp nội dung từng file vào file tổng\n",
        "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    for text_file in text_files:\n",
        "        file_path = os.path.join(text_dir, text_file)\n",
        "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
        "            content = infile.read()\n",
        "            outfile.write(f\"=== {text_file} ===\\n\")  # Ghi tiêu đề mỗi file\n",
        "            outfile.write(content + \"\\n\\n\")\n",
        "\n",
        "print(f\"✅ Đã gộp {len(text_files)} file vào: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_2ztKeOtYp_"
      },
      "source": [
        "###Tiền xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sJsP61trTM6",
        "outputId": "505f452b-e2c3-488b-b166-4f53cedaca30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker, emoji\n",
            "Successfully installed emoji-2.14.1 pyspellchecker-0.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk beautifulsoup4 emoji pyspellchecker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaHCH92hsMEH"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.rmtree('/root/nltk_data', ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvxz5LxpsNLf",
        "outputId": "1d18f243-752d-4b93-b051-81b4daf815b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gcUgWIDleAB"
      },
      "outputs": [],
      "source": [
        "with open('/all_texts_combined.txt', 'r', encoding='utf-8') as f:\n",
        "    content = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_pdcI5FxLbl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Đọc CSV\n",
        "df = pd.read_csv(\"/test_questions.csv\")\n",
        "\n",
        "# Lấy cột \"question\" và ghi từng dòng vào file text\n",
        "with open(\"questions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for q in df[\"Question\"]:\n",
        "        f.write(q.strip() + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO6mvCyCq6Gu"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
